<section id="research">
  <h2>Research Experience</h2>

  <!-- Graduate Research Assistant (Parent) -->
  <div class="exp">
    <h3>Graduate Research Assistant <span class="org">— University of Southern California</span></h3>

    <!-- GLAMOR (Child) -->
    <div class="subexp">
      <h4>GLAMOR Lab</h4>
      <ul>
        <li>At the GLAMOR Lab, our research aims to advance robots’ perception, reasoning, and control capabilities.</li>
        <li>Developing a simulation testing suite to evaluate robot learning policies.</li>
      </ul>
    </div>

    <!-- LiRA (Child) -->
    <div class="subexp">
      <h4>LiRA Lab</h4>
      <ul>
        <li>Developing algorithms for robot learning, safe and efficient human-robot interaction, and multi-agent systems.</li>
        <li>Building a robust VLA model for imitation learning that can adapt to diverse environments.</li>
      </ul>
    </div>
  </div>

  <!-- ETRI -->
  <div class="exp">
    <h3>Intern <span class="org">— Electronics and Telecommunications Research Institute (ETRI)</span></h3>
    <p class="meta">Visual Intelligence Research Section (Advisor: Dr. Hyung-Il Kim)</p>
    <ul>
      <li>Led a project focused on banner detection and text recognition affected by distortion, occlusion, and noise.</li>
      <li>Analyzed large-scale language and image data from sources including AI-Hub, ICDAR, MJ, ST, KoBERT, and IIIT5K.</li>
      <li>Identified a decrease in model accuracy during simultaneous multi-language training (English/Korean/Chinese).</li>
      <li>Built an English-Korean bilingual dataset and developed a banner detection model based on TextBPN and CRAFT.</li>
      <li>Constructed a contrastive learning structure to distinguish similar texts; applied CLIP and prompt tuning for inference/correction.</li>
      <li>Contributed to IEIE publications/posters; continuing follow-up research and paper writing.</li>
    </ul>

    <div class="repo-links">
      <a href="https://github.com/LeeHakHo/ETRI_Project" target="_blank" rel="noopener noreferrer">
        <svg style="width:20px; height:20px; vertical-align: middle; margin-right:6px;" viewBox="0 0 16 16" aria-hidden="true">
          <path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
        </svg>
        Banner Detection
      </a>

      <a href="https://github.com/LeeHakHo/clipstr" target="_blank" rel="noopener noreferrer">
        <svg style="width:20px; height:20px; vertical-align: middle; margin-right:6px;" viewBox="0 0 16 16" aria-hidden="true">
          <path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
        </svg>
        CLIPSTR
      </a>
    </div>
  </div>

  <!-- Gachon Visual AI Lab -->
  <div class="exp">
    <h3>Undergraduate Research Assistant <span class="org">— Visual AI Lab, Gachon University</span></h3>
    <p class="meta">Advisor: Prof. Jungchan Cho</p>
    <ul>
      <li>Analyzed human states and fine-grained behaviors in videos for comprehensive situational understanding.</li>
      <li>Developed a Video Swin Transformer model for behavior classification using VIRAT dataset.</li>
      <li>Performed augmentation/transforms on Stanford40 &amp; VIRAT with PyTorch/OpenCV; compared models with Grad-CAM, W&amp;B, Matplotlib.</li>
      <li>Held weekly seminars for paper reviews and progress discussions.</li>
    </ul>
  </div>
</section>
