<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Hyeonho OH | AI Researcher</title>
  <style>
    body {
      font-family: 'Helvetica Neue', sans-serif;
      margin: 0;
      padding: 0;
      color: #2c3e50;
      background: #f4f6f9;
      line-height: 1.6;
    }
        header {
      text-align: center;
      padding: 70px 20px 50px 20px; 
      background: linear-gradient(120deg, #0a3d62, #3c6382);
      color: white;
    }
    header img {
      width: 140px; 
      height: 140px;
      border-radius: 50%;
      border: 4px solid white;
      margin-bottom: 20px;
    }

    section {
      max-width: 850px; 
      margin: 40px auto;
      padding: 20px 30px;
      background: white; 
      border-radius: 12px; 
      box-shadow: 0 4px 12px rgba(0,0,0,0.08);
    }
    section h2 {
      border-bottom: 2px solid #dcdde1;
      padding-bottom: 8px; 
      margin-bottom: 20px;
      font-weight: 600;
      font-size: 1.4em;
    }

    
    .links a {
      display: inline-block;
      margin: 8px 12px;
      padding: 10px 20px;
      text-decoration: none;
      background: #0a3d62; 
      color: white;
      border-radius: 25px; 
      font-size: 0.95em;
      transition: background 0.3s, transform 0.2s; 
    }
    .links a:hover {
      background: #1e6091;
      transform: translateY(-2px); 
    }

    /* ✅ [변경] 푸터: 흰색 → 다크 블루 */
    footer {
      text-align: center;
      background: #0a3d62;
      color: white;
      padding: 20px;
      margin-top: 50px;
      font-size: 0.9em;
    }
  </style>
</head>
<body>
  <header>
    <img src="hyeonhoo_photo.jpeg" alt="Profile Photo">
    <h1>Hyeonho Oh</h1>
    <p>AI Researcher | Computer Vision | Robotics</p>
  </header>

  <section id="about">
    <h2>About Me</h2>
    <p>
      Hi there! I am a master student in Computer Science at the University of Southern Califorina. I received my B.Eng. degree in Department of AI & Software from Gachon University, Korea in 2025.
    </p>
  </section>

  <section id="interests">
    <h2>Research Interests</h2>
    <ul>
      <li>Vision-Language Models</li>
      <li>Visual Understanding</li>
      <li>Application of Visual Information to Hardware System</li>
    </ul>
  </section>
  
<section id="research">
    <h2>Research Experience</h2>
      <b>Electronics and Telecommunications Research Institute (ETRI)</b>
        <br>Intern, Visual Intelligence Research Section (Advisor: Dr. Hyung-Il Kim)	
    <ul>
      <li>Led a project focused on banner detection and text recognition affected by distortion, occlusion, and noise</li>
      <li>Analyzed large-scale language and image data from sources including AI-Hub, ICDAR, MJ, ST, Kobert, and IIIT5K</li>
      <li>Identified a decrease in model accuracy during simultaneous multiple-language training (English/Korean/Chinese)</li>
      <li>Built an English-Korean bilingual dataset and developed a banner detection model from TextBPN and CRAFT</li>
      <li>Constructed a contrast learning structure to distinguish similar texts in images; applied large-scale image-text CLIP models and prompt tuning techniques for text inference and correction from images</li>
      <li>Contributed to paper publications and poster presentations at The Institute of Electronics & Information Engineers (IEIE); currently engaged in follow-up research and paper writing</li>
      <a href="https://github.com/LeeHakHo/ETRI_Project" style="text-decoration: none;" >
    <!-- 1. 아이콘 코드 -->
    <svg style="width:20px; height:20px; vertical-align: middle; margin-right: 6px;" viewBox="0 0 16 16" version="1.1" aria-hidden="true"><path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path></svg>
    <!-- 2. 텍스트 -->
    Banner Detection
      <a href="https://github.com/LeeHakHo/clipstr" style="text-decoration: none;" >
    <!-- 1. 아이콘 코드 -->
    <svg style="width:20px; height:20px; vertical-align: middle; margin-right: 6px;" viewBox="0 0 16 16" version="1.1" aria-hidden="true"><path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path></svg>
    <!-- 2. 텍스트 -->
    CLIPSTR
</a>
    </ul>
      <b>Visual AI Lab at Gachon University</b>
      <br>Undergraduate Research Assistant (Advisor: Prof. Jungchan Cho)
    <ul>
      <li>Worked in a vision AI lab that analyzed human states and detailed behaviors in videos for comprehensive situational understanding</li>
      <li>Developed Video Swin Transformer model to classify human behavior among video objects using VIRAT dataset</li>
      <li>Performed data augmentation and transformation on the Stanford40 and VIRAT dataset using PyTorch and OpenCV and implemented ResNet-based and Transformer-based models; compared, analyzed, and visualized the results using Grad-CAM, Weights & Biases(wandb) and Matplotlib</li>
      <li>Conducted weekly seminars to review papers and discuss progress with researchers and the professor</li>
    </ul>
  </section>

  <section id="projects">
    <h2>Project Experience</h2>
      <b>New Frontiers for Zero-shot for Image Captioning Evaluation (NICE) Challenge, CVPR'23</b>
    <ul>
      <li>Enhanced the performance of OFA and mPLUG zero-shot captioning models, achieving improved accuracy on evaluation datasets, including COCOcaptions, Flickr30k, Conceptual Captions (CC3M), and LAION</li>
      <li>Improved the model score using prompt learning and conducted experiments to compare captioning models</li>
      <li>Refined base model predictions by feeding generated sentences into a secondary model to enhance accuracy</li>
      <li>Utilized prompt tuning to guide the CLIP model in identifying appropriate words</li>
    </ul>
      <b>Korean Text Recognition Challenge, Korean Ministry of Science and ICT</b>
    <ul>
      <li>Increased OCR generalization performance through an improved TRBA model</li>
      <li>Improved the ResNet-based feature extractor with a SENet-based architecture</li>
      <li>Applied sampling based on character frequency and employed ensemble methods to increase performance</li>
      <li>Identified weak characters using Grad-CAM, built a focused dataset and improved accuracy through fine-tuning</li>
      <a href="https://github.com/LeeHakHo/TSBA" style="text-decoration: none;" >
      <!-- 1. 아이콘 코드 -->
      <svg style="width:20px; height:20px; vertical-align: middle; margin-right: 6px;" viewBox="0 0 16 16" version="1.1" aria-hidden="true"><path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path></svg>
      <!-- 2. 텍스트 -->
      TSBA
      </a>
    </ul>
      <b>Data Science Academic Presentation Contest, Gachon University</b>
    <ul>
      <li>Predicted multiple matches in the Qatar World Cup, including Argentina's win, using match records and player stats-based machine learning algorithms</li>
      <li>Combined and sampled datasets containing player attributes, match statistics, and player evaluations for comprehensive analysis</li>
      <li>Visualized data and results using Matplotlib, scatter plots, and correlation maps</li>
      <a href="https://github.com/LeeHakHo/WorldcupPrediction" style="text-decoration: none;" >
      <!-- 1. 아이콘 코드 -->
      <svg style="width:20px; height:20px; vertical-align: middle; margin-right: 6px;" viewBox="0 0 16 16" version="1.1" aria-hidden="true"><path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path></svg>
      <!-- 2. 텍스트 -->
     Worldcup Prediction
      </a>
    </ul>
      <b>Graduation Project: Object Detection & Human-Object Interaction (HOI), Gachon Univ.</b>
    <ul>
      <li>Built models to detect scene events between people and objects based on YOLO, FairMOT, and HOTR</li>
      <li>Utilized YOLOv5 and FairMOT for real-time multi-person and object detection</li>
      <li>Created a video dataset representing interactions between people and objects</li>
      <li>Employed HOTR (Human-Object Interaction Transformer) to detect interactions</li>
      <li>Constructed a custom dataset and implemented server/client structure</li>
    </ul>
  </section>
  
<section id="publications">
    <h2>Selected Publications</h2>
    <ul>
        <li>Oh, H.H., Yun, J.S., Bae, Y.S., & Kim, H.I. "<a href="https://drive.google.com/file/d/1x9T30t-aZ0LIQBByVvGI1xP9pWmagzGE/view?usp=drive_link" target="_blank">Investigating Performance of Text Classification Learned with Multi-language for Scene Text Recognition </a>." <i>In Proceedings of the IEIE Summer Conference</i>, pp. 1058-1061, Jun. 2023.</li>
    </ul>
</section>
  
  <section id="Awards">
    <h2>Awards</h2>
    <ul>
      <li><b>Outstanding Talent Award</b> issued by Gachon university</li>
      <li><b>Second Place of Korean Characters OCR AI Contest</b> issued by Korean Ministry of Science and ICT </li>
      <li><b>First Place, Academic Oral Presentation with Data Science</b> issued by Gachon University</li>
    
    
    </ul>
  </section>
  
  <section id="links">
    <h2>Links</h2>
    <div class="links">
      <a href= "https://leehakho.notion.site/Paper-Review-5390717b42774e9b9ba3d06196903019" target="_blank">Paper Record</a>
      <a href="https://github.com/LeeHakHo" target="_blank">GitHub</a>
      <a href="https://www.linkedin.com/in/hyeonhoo" target="_blank">LinkedIn</a>
      <a href="mailto:xyeonxo@gmail.com">Email</a>
    </div>
  </section>

  <footer>
    <p>© 2025 Hyeonho Oh</p>
  </footer>
</body>
</html>
